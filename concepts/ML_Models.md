# Machine Learning 

[Every ml model in 15 mins](https://www.youtube.com/watch?v=E0Hmnixke2g)

## Supervised Learning (Labelled)

# - Regression

#### You're solving for y using x(s).
##### - Linear Regression (predicting a y based on a x)

y = 1/2a
y= -9c
y=-7b

what is impacting more? C because it negativelty impacts more. Therefore, if we wanna save on resources than we can skip a as it has the least impact

- Random Forest (uses bagging concept to built on top of multiple decision trees and combine their effects)
- Xgboost (uses boosting to use mutiple dec tress in sequence rather than in parallel as done in random forest

 # - Classification

- Logistic Regression
  </br>

![image](https://github.com/user-attachments/assets/9420231f-61fc-4d2f-b8ea-65d600c7d47f)

</br>

- Decision Tree
 </br>

![image](https://github.com/user-attachments/assets/3cdb370e-d903-43cc-a2ec-90ac15119e1b)
</br>

-k-nearest (both classification and regression)

For classfication, 

![image](https://github.com/user-attachments/assets/8358dbd3-9aca-45e1-a086-54061cbe260a)
</br>
x1 = weigth 
x2 = height 

The person belong a gender whose weight and height are nearest to its k neighbours. 


For regression,

the weight of a person would be average of k person nearest to him

Too high k overfits, too low underfits, something in middle is the best

- SVM
![image](https://github.com/user-attachments/assets/b1c9b7f7-8422-4bf0-82f2-2648ec8f02e8)
Kernel Functions in SVM help make better boundaries by turning features into more complex features
we use bm = weight/height

- Naive Bayes
  Probability of words appearing are not dependent on another

- Neural Network
</br>


 
## Unsupervised Learning (Unlabelled)
- K means
  
- dimentionality reduction
  
- principle component analysis
can use a combition of two features when they contradict with one another.

